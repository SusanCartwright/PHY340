#Hypothesis Testing, Confidence Intervals and Limits

Using &chi;<sup>2</sup> or maximum likelihood, we can fit any function to any dataset &ndash; that is, we can find the set of parameters that best describes the data.  This is not the same thing as saying that this parameter set describes the data **well**.  (If you try to fit an exponential distribution with a straight line, you will get the *best* fit, but in most cases it won't be a *good* fit).  This is not very satisfactory.  We need to be able to answer questions like:

* Is model A a good fit to my data?
* Is model A a better fit to my data than model B?
* Can I reject model A on the basis of my data, and with what level of confidence?

These are closely related questions, and come under the broad heading of **hypothesis testing**.

##&chi;<sup>2</sup> and goodness of fit (see [this notebook](chisquare.ipynb))

The &chi;<sup>2</sup> of a fit is the sum of the squared **residuals** (the residual is the difference between the measured and fitted values, *y* &minus; *y*<sub>fit</sub>), *in units of the error on the measured value*, &chi;<sup>2</sup> = &sum;(*y*<sub>i</sub> - *y*<sub>fit</sub>)<sup>2</sup>/&sigma;<sub>i</sub><sup>2</sup>.  A &chi;<sup>2</sup> fit determines the parameters of the function to be fitted by minimising the &chi;<sup>2</sup>: it is the generalisation of the least-squares fit to the case where the error bars of the points are not all equal (see [this notebook](fits2.ipynb)).  By the same logic, the **absolute** value of &chi;<sup>2</sup><sub>min</sub> is a measure of the quality of that best fit &ndash; the larger the &chi;<sup>2</sup>, the further are the measured points from the fitted function, and hence the less good is the fit.

If the errors on the measured points are Gaussian, the quantity (*y*<sub>i</sub> - *y*<sub>true</sub>)/&sigma;<sub>i</sub> should average to about 1 (since &sigma; is the probable error of *y*), so if *y*<sub>fit</sub> is a good estimate of *y*<sub>true</sub>, we expect &chi;<sup>2</sup>/*N*<sub>dof</sub>, where *N*<sub>dof</sub> is the number of degrees of freedom in the fit, to be about 1.  In fact, for Gaussian errors the &chi;<sup>2</sup> distribution has a mean of *N*<sub>dof</sub> and a variance of 2*N*<sub>dof</sub>: this means that the probability of &chi;<sup>2</sup>/*N*<sub>dof</sub> being greater than, say, 2 decreases as *N*<sub>dof</sub> increases.  Books on statistics have tables and/or plots of the probability of a given &chi;<sup>2</sup> value as a function of *N*<sub>dof</sub>, but these days it is much easier to use a standard computer program such as Python's [**stats.chi2**](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html) to calculate it.

As noted above, the definition of &chi;<sup>2</sup> assumes Gaussian errors.  If the underlying errors are not Gaussian (for example, if you are fitting a histogram with limited statistics, so that the Poisson distribution of the numbers of entries is not well approximated by the Gaussian), you are still justified in using &chi;<sup>2</sup> minimisation to fit your parameters, but the probability you deduce from the value of &chi;<sup>2</sup> is not reliable.

##Maximum likelihood fitting

In many cases, &chi;<sup>2</sup> minimisation is used to fit functions even when the underlying distributions are not Gaussian, because it's a straightforward procedure implemented in many standard program libraries (though it should be noted that for non-linear fits, the effectiveness of the minimisation procedure itself is not always guaranteed, owing to the possible existence of local minima; as a result, there are *many* different minimisation methods available, all with their own advantages and disadvantages: see the list of available methods in the [scipy.optimize.minimize](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) documentation for some examples).  However, in cases where Gaussian errors clearly do not apply, it may be more appropriate to use [**maximum likelihood**](http://mathworld.wolfram.com/MaximumLikelihood.html) instead.

Maximum likelihood estimators, as their name suggests, work by maximising the **likelihood** &Lscr;.  The likelihood of a single measured point is the probability that the measured value would be observed, assuming that the true value is given by the function to be fitted: *P*(*y*<sub>i</sub>|*f*<sub>**a**</sub>(*x*<sub>i</sub>)), where *f* is the function and the vector **a** contains its parameters.  The overall likelihood of the dataset, &Lscr;, is the product of all these probabilities, &prod;*P*<sub>i</sub>.  It is usual to work with ln &Lscr; instead of &Lscr; directly, because taking the log converts the product into a sum, and sums are easier to deal with; it is also usual to minimise &minus;ln &Lscr; instead of maximising ln &Lscr;, because there are more well-studied algorithms available for minimisation than for maximisation.

Maximum likelihood is harder to set up than &chi;<sup>2</sup> minimisation, because you need to specify a probability density function for your data as well as the function to be fitted.  Another issue is that the uncertainty in the fitted parameters is less simple to estimate.  On the other hand, maximum likelihood is much more flexible and can deal effectively with more complicated data.  It is widely used in many fields, such as particle physics: see, for example, Chapter 6 of Glen Cowan's *Statistical Data Analysis*.

##Hypothesis testing and the F-test (see [this notebook](hypothesis_test.ipynb))

Formal hypothesis testing compares two models for the data sample: the **null hypothesis**, which is essentially a statement that nothing interesting is present (e.g. no Higgs boson, no effect of drug X on disease Y, no planet in orbit around star S), and an **alternative hypothesis** which is usually that a signal of some sort is present (e.g. Higgs boson mass peak, improvement in patient condition following administration of drug X, periodic reduction in brightness of star S caused by transit of planet).  The aim of hypothesis testing is to determine whether the alternative hypothesis fits the data better than the null hypothesis, and consequently to decide whether or not to reject the null hypothesis (i.e. to conclude that there is a real effect).  It is never possible to *prove* the null hypothesis &ndash; it is always possible that there *is* a signal, but at a level so low that it is not visible &ndash; but it is possible to *reject* it.

###Procedure for hypothesis testing

Hypothesis testing is a matter of probabilities, and probabilities are very susceptible to bias (this is why medical trials are, where possible, conducted [double-blind](https://en.wikipedia.org/wiki/Blind_experiment) &ndash; neither the patient nor the doctor knows whether the patient is receiving the trial drug or the control, because it has been shown that if either does know, their unconscious expectations affect the result).  Therefore, it is essential to have a well-defined protocol when setting up a hypothesis test.

1. Define the null hypothesis and the alternative hypothesis (if you have not carefully specified exactly what you plan to test, the results will be difficult or impossible to interpret).

2. Consider the properties of your dataset (e.g. are the measurement errors Gaussian?) and your hypotheses (e.g. is one nested within the other?), and decide on an appropriate **test statistic** (e.g. &Delta;&chi;<sup>2</sup> or &Lscr;<sub>2</sub>/&Lscr;<sub>1</sub>).

3. Choose your threshold for rejecting the null hypothesis, i.e. the level of (im)probability at which you will decide that the null hypothesis is untenable.  Particle physics conventionally uses 5&sigma;, but this is very demanding: many fields use 5% or 1%.

4. Analyse your data using the method you chose in (2), and interpret it according to the threshold you chose in (3).

**It is very important that you carry out the steps in the correct order**, and in particular that you do not "move the goalposts" by changing your threshold after you have looked at the results.  It *is*, however, OK to decide, on the basis of your results, that your choices in steps 2 and 3 were not optimal, change them, and then use the changed criteria to analyse *an independent dataset*.  The key word here is *independent*: using your changed criteria to analyse the dataset that prompted you to change the criteria in the first place is circular reasoning.

###Comparing "nested" models: the F-test

It is often the case that the alternative hypothesis, model 2, is an addition to the null hypothesis, model 1, in the sense that the function describing it is the same function that describes the null hypothesis, plus an extra bit (for example, you may fit the background with a polynomial, and the "signal" data, say a Higgs boson mass peak, with the same polynomial plus a Gaussian for the signal).  In this case, a simple difference in &chi;<sup>2</sup> is not appropriate, because the fits are correlated: some of the variance, that caused by the background, is common to both fits.  In this case, a suitable test statistic is the **F-test**, which is constructed by taking the "improvement" in &chi;<sup>2</sup>, (&chi;<sup>2</sup><sub>1</sub> &minus; &chi;<sup>2</sup><sub>2</sub>)/(*p*<sub>2</sub> &minus; *p*<sub>1</sub>) (the difference in &chi;<sup>2</sup>, divided by the difference in number of parameters) and dividing it by the reduced &chi;<sup>2</sup> for model 2, &chi;<sup>2</sup><sub>2</sub>/(*N* &minus; *p*<sub>2</sub>).  This quantity is distributed according to the [F-distribution](https://en.wikipedia.org/wiki/F-distribution), and the interesting statistic is the probability of getting a value at least as large as the one you have calculated, assuming that the additional parameters in the alternative hypothesis do *not* significantly improve the fit (i.e. that the alternative hypothesis is not required by the data).  (*Note that adding an additional parameter will always improve the fit to some extent: the question is whether the improvement is significant.*)  If the probability you calculate is less than the threshold you defined in step 3, the null hypothesis is rejected.

###Comparing independent models

If you are comparing independent models (for example, is this spectral line better described by a Gaussian or a Lorentzian?) then you can use the difference in &chi;<sup>2</sup>, or (if using maximum likelihood, the likelihood ratio, &Lscr;<sub>2</sub>/&Lscr;<sub>1</sub>) as a test statistic.

###Comparing two datasets: the KS-test

Sometimes you want to compare two datasets, instead of a dataset and a model.  A useful test statistic in this case is the [**Kolmogorov-Smirnov**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) (KS) test, which is based on the maximum difference between the normalised cumulative distributions of the two samples.  As with the F-test, there is an underlying probability distribution, and you can look up in tables, or use a program to return, the probability that the value of the KS statistic is at least as large as the one you calculate, assuming that the two samples are in fact drawn from the same distribution.  If that probability is less than your predetermined threshold, you can reject the null hypothesis (that the two samples *are* from the same distribution) in favour of teh alternative hypothesis (that there is something different about sample 2). 

You do not need a model to use this form of the KS test, but the KS test can equally well be used with a model prediction replacing one of the test samples.