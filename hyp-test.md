#Hypothesis Testing, Confidence Intervals and Limits

Using &chi;<sup>2</sup> or maximum likelihood, we can fit any function to any dataset &ndash; that is, we can find the set of parameters that best describes the data.  This is not the same thing as saying that this parameter set describes the data **well**.  (If you try to fit an exponential distribution with a straight line, you will get the *best* fit, but in most cases it won't be a *good* fit).  This is not very satisfactory.  We need to be able to answer questions like:

* Is model A a good fit to my data?
* Is model A a better fit to my data than model B?
* Can I reject model A on the basis of my data, and with what level of confidence?

These are closely related questions, and come under the broad heading of **hypothesis testing**.

##&chi;<sup>2</sup> and goodness of fit (see [this notebook](chisquare.ipynb))

The &chi;<sup>2</sup> of a fit is the sum of the squared **residuals** (the residual is the difference between the measured and fitted values, *y* &minus; *y*<sub>fit</sub>), *in units of the error on the measured value*, &chi;<sup>2</sup> = &sum;(*y*<sub>i</sub> - *y*<sub>fit</sub>)<sup>2</sup>/&sigma;<sub>i</sub><sup>2</sup>.  A &chi;<sup>2</sup> fit determines the parameters of the function to be fitted by minimising the &chi;<sup>2</sup>: it is the generalisation of the least-squares fit to the case where the error bars of the points are not all equal (see [this notebook](fits2.ipynb)).  By the same logic, the **absolute** value of &chi;<sup>2</sup><sub>min</sub> is a measure of the quality of that best fit &ndash; the larger the &chi;<sup>2</sup>, the further are the measured points from the fitted function, and hence the less good is the fit.

If the errors on the measured points are Gaussian, the quantity (*y*<sub>i</sub> - *y*<sub>true</sub>)/&sigma;<sub>i</sub> should average to about 1 (since &sigma; is the probable error of *y*), so if *y*<sub>fit</sub> is a good estimate of *y*<sub>true</sub>, we expect &chi;<sup>2</sup>/*N*<sub>dof</sub>, where *N*<sub>dof</sub> is the number of degrees of freedom in the fit, to be about 1.  In fact, for Gaussian errors the &chi;<sup>2</sup> distribution has a mean of *N*<sub>dof</sub> and a variance of 2*N*<sub>dof</sub>: this means that the probability of &chi;<sup>2</sup>/*N*<sub>dof</sub> being greater than, say, 2 decreases as *N*<sub>dof</sub> increases.  Books on statistics have tables and/or plots of the probability of a given &chi;<sup>2</sup> value as a function of *N*<sub>dof</sub>, but these days it is much easier to use a standard computer program such as Python's [**stats.chi2**](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html) to calculate it.

As noted above, the definition of &chi;<sup>2</sup> assumes Gaussian errors.  If the underlying errors are not Gaussian (for example, if you are fitting a histogram with limited statistics, so that the Poisson distribution of the numbers of entries is not well approximated by the Gaussian), you are still justified in using &chi;<sup>2</sup> minimisation to fit your parameters, but the probability you deduce from the value of &chi;<sup>2</sup> is not reliable.

##Maximum likelihood fitting

In many cases, &chi;<sup>2</sup> minimisation is used to fit functions even when the underlying distributions are not Gaussian, because it's a straightforward procedure implemented in many standard program libraries (though it should be noted that for non-linear fits, the effectiveness of the minimisation procedure itself is not always guaranteed, owing to the possible existence of local minima; as a result, there are *many* different minimisation methods available, all with their own advantages and disadvantages: see the list of available methods in the [scipy.optimize.minimize](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) documentation for some examples).  However, in cases where Gaussian errors clearly do not apply, it may be more appropriate to use [**maximum likelihood**](http://mathworld.wolfram.com/MaximumLikelihood.html) instead.

Maximum likelihood estimators, as their name suggests, work by maximising the **likelihood** &Lscr;.  The likelihood of a single measured point is the probability that the measured value would be observed, assuming that the true value is given by the function to be fitted: *P*(*y*<sub>i</sub>|*f*<sub>**a**</sub>(*x*<sub>i</sub>)), where *f* is the function and the vector **a** contains its parameters.  The overall likelihood of the dataset, &Lscr;, is the product of all these probabilities, &prod;*P*<sub>i</sub>.  It is usual to work with ln &Lscr; instead of &Lscr; directly, because taking the log converts the product into a sum, and sums are easier to deal with; it is also usual to minimise &minus;ln &Lscr; instead of maximising ln &Lscr;, because there are more well-studied algorithms available for minimisation than for maximisation.

Maximum likelihood is harder to set up than &chi;<sup>2</sup> minimisation, because you need to specify a probability density function for your data as well as the function to be fitted.  Another issue is that the uncertainty in the fitted parameters is less simple to estimate.  On the other hand, maximum likelihood is much more flexible and can deal effectively with more complicated data.  It is widely used in many fields, such as particle physics: see, for example, Chapter 6 of Glen Cowan's *Statistical Data Analysis*.

##Hypothesis testing and the F-test (see [this notebook](hypothesis_test.ipynb))

Formal hypothesis testing compares two models for the data sample: the **null hypothesis**, which is essentially a statement that nothing interesting is present (e.g. no Higgs boson, no effect of drug X on disease Y, no planet in orbit around star S), and an **alternative hypothesis** which is usually that a signal of some sort is present (e.g. Higgs boson mass peak, improvement in patient condition following administration of drug X, periodic reduction in brightness of star S caused by transit of planet).  The aim of hypothesis testing is to determine whether the alternative hypothesis fits the data better than the null hypothesis, and consequently to decide whether or not to reject the null hypothesis (i.e. to conclude that there is a real effect).  It is never possible to *prove* the null hypothesis &ndash; it is always possible that there *is* a signal, but at a level so low that it is not visible &ndash; but it is possible to *reject* it.

###Procedure for hypothesis testing

Hypothesis testing is a matter of probabilities, and probabilities are very susceptible to bias (this is why medical trials are, where possible, conducted [double-blind](https://en.wikipedia.org/wiki/Blind_experiment) &ndash; neither the patient nor the doctor knows whether the patient is receiving the trial drug or the control, because it has been shown that if either does know, their unconscious expectations affect the result).  Therefore, it is essential to have a well-defined protocol when setting up a hypothesis test.

1. Define the null hypothesis and the alternative hypothesis (if you have not carefully specified exactly what you plan to test, the results will be difficult or impossible to interpret).

2. Consider the properties of your dataset (e.g. are the measurement errors Gaussian?) and your hypotheses (e.g. is one nested within the other?), and decide on an appropriate **test statistic** (e.g. &Delta;&chi;<sup>2</sup> or &Lscr;<sub>2</sub>/&Lscr;<sub>1</sub>).

3. Choose your threshold for rejecting the null hypothesis, i.e. the level of (im)probability at which you will decide that the null hypothesis is untenable.  Particle physics conventionally uses 5&sigma;, but this is very demanding: many fields use 5% or 1%.

4. Analyse your data using the method you chose in (2), and interpret it according to the threshold you chose in (3).

**It is very important that you carry out the steps in the correct order**, and in particular that you do not "move the goalposts" by changing your threshold after you have looked at the results.  It *is*, however, OK to decide, on the basis of your results, that your choices in steps 2 and 3 were not optimal, change them, and then use the changed criteria to analyse *an independent dataset*.  The key word here is *independent*: using your changed criteria to analyse the dataset that prompted you to change the criteria in the first place is circular reasoning.

###Comparing "nested" models: the F-test

It is often the case that the alternative hypothesis, model 2, is an addition to the null hypothesis, model 1, in the sense that the function describing it is the same function that describes the null hypothesis, plus an extra bit (for example, you may fit the background with a polynomial, and the "signal" data, say a Higgs boson mass peak, with the same polynomial plus a Gaussian for the signal).  In this case, a simple difference in &chi;<sup>2</sup> is not appropriate, because the fits are correlated: some of the variance, that caused by the background, is common to both fits.  In this case, a suitable test statistic is the **F-test**, which is constructed by taking the "improvement" in &chi;<sup>2</sup>, (&chi;<sup>2</sup><sub>1</sub> &minus; &chi;<sup>2</sup><sub>2</sub>)/(*p*<sub>2</sub> &minus; *p*<sub>1</sub>) (the difference in &chi;<sup>2</sup>, divided by the difference in number of parameters) and dividing it by the reduced &chi;<sup>2</sup> for model 2, &chi;<sup>2</sup><sub>2</sub>/(*N* &minus; *p*<sub>2</sub>).  This quantity is distributed according to the [F-distribution](https://en.wikipedia.org/wiki/F-distribution), and the interesting statistic is the probability of getting a value at least as large as the one you have calculated, assuming that the additional parameters in the alternative hypothesis do *not* significantly improve the fit (i.e. that the alternative hypothesis is not required by the data).  (*Note that adding an additional parameter will always improve the fit to some extent: the question is whether the improvement is significant.*)  If the probability you calculate is less than the threshold you defined in step 3, the null hypothesis is rejected.

###Comparing independent models

If you are comparing independent models (for example, is this spectral line better described by a Gaussian or a Lorentzian?) then you can use the difference in &chi;<sup>2</sup>, or (if using maximum likelihood, the likelihood ratio, &Lscr;<sub>2</sub>/&Lscr;<sub>1</sub>) as a test statistic.

###Comparing two datasets: the KS-test

Sometimes you want to compare two datasets, instead of a dataset and a model.  A useful test statistic in this case is the [**Kolmogorov-Smirnov**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) (KS) test, which is based on the maximum difference between the normalised cumulative distributions of the two samples.  As with the F-test, there is an underlying probability distribution, and you can look up in tables, or use a program to return, the probability that the value of the KS statistic is at least as large as the one you calculate, assuming that the two samples are in fact drawn from the same distribution.  If that probability is less than your predetermined threshold, you can reject the null hypothesis (that the two samples *are* from the same distribution) in favour of the alternative hypothesis (that there is something different about sample 2). 

You do not need a model to use this form of the KS test, but the KS test can equally well be used with a model prediction replacing one of the test samples.

###Look-elsewhere effect

The [look-elsewhere effect](http://cms.web.cern.ch/news/should-you-get-excited-your-data-let-look-elsewhere-effect-decide) is an effect that has to be taken into consideration if your alternative hypothesis does not specify the *location* of the expected signal.  The probability that a large statistical fluctuation takes place in one of many bins of a histogram is much large than the probability that it takes place in one specific bin.  Therefore, you need to consider, when calculating the probability of the null hypothesis, not the probability that you would get a fluctuation this large *here* but that you would get a fluctuation this large *anywhere*.  If the potential signal could occur over a wide range of possible locations, this can make a very large difference to your probability.

##Confidence intervals, limits and exclusions (see [this notebook](conf-lim.ipynb))

If we know the underlying probability distribution of our data, it is usually easy to calculate the probability that a measurement will lie within a given distance of the true population mean, and therefore to calculate a *confidence interval* such that repeated measurements will lie within this interval X% of the time, where X is some specified number (68.3% for a 1&sigma; error bar).  However, this is not what we need to know: generally we do *not* know the true value, and we want our error bar to tell us something about the location of the true value with respect to the measured value, rather than the reverse.

Fortunately, this is straightforward for simple Gaussian errors: the Gaussian pdf depends on (*x* &minus; *&mu;*)<sup>2</sup>, which is symmetric under exchange of the measured value *x* and the true population mean *&mu;*.  By the central limit theorem, measured quantities which are averages of several individual measurements are approximately Gaussian, so this is often valid.  However, there are cases which present more of a problem:

* the bin contents of histograms have a Poisson distribution, and if the number of entries per bin is small this is significantly different from a Gaussian;

* the measured quantity may be capable of taking values which are unphysical, especially in the presence of background (for example, the DM-ICE data we analysed [earlier](hypothesis_test.ipynb) gave a best fit oscillation amplitude which was negative: a negative amplitude would require the Earth's direction of motion to be reversed, and is therefore unphysical);

* the pdf of a physics result which is obtained indirectly from the measured data (e.g. it is a fit parameter) may be far from Gaussian.

In these cases, obtaining accurate confidence intervals may present much more of a problem.

###Frequentist statistics

Classical statistics is **frequentist**: that is, it is assumed that the experiment could be repeated an indefinite number of times, and that the actual experimental results are a random sample from this (hypothetical) set of repetitions, all drawn from the same true distribution.  The true parameters, though unknown, are fixed.  The aim in defining an X% confidence interval is that in X% of the set of repetitions, the confidence interval would contain the true parameter; in defining a Y% upper or lower limit, it is that in Y% of repetitions the true parameter would *not* be located in the excluded region.

The method for defining confidence intervals in frequentist analysis is set out in [the notebook](conf-lim.ipynb).  Cases where there are complications such as unphysical regions of parameter space can be dealt with using [the Feldman-Cousins prescription](http://arxiv.org/abs/physics/9711021), which is standard practice in particle physics but applicable to any analysis with small signals (generally, when signal numbers are large it becomes increasingly safe to assume Gaussian errors, as the Poisson is well approximated by the Gaussian at large population mean &mu;).  A problem in frequentist statistics is the treatment of **nuisance parameters**, which are parameters whose values are not precisely known (so they have to be fitted), but which are irrelevant to the physics.  In principle, confidence intervals should be defined such that they will give the appropriate probability after allowing the nuisance parameters to vary over the full possible range.  In practice, one attempts to define test statistics with minimal sensitivity to nuisance parameters, such as the **profile likelihood ratio**.

###Bayesian statistics

In contrast to frequentist statistics, Bayesian inference treats the data as fixed, and defines a probability density function *for the true parameter values*.  This is not a frequentist probability (one assumes that the true parameters do really have a fixed value), but instead quantifies the experimenter's **degree of belief** in the value in question.  The equivalent of a confidence interval in Bayesian statistics is a **credible interval**, which should be defined such that for a confidence level of X, there is a probability X that the true value will lie in the defined interval.  Note that this is *not* equivalent to the frequentist definition, even though it appears very similar.

Bayesian statistics is so called because it relies on **Bayes' theorem** of conditional probability: *P*(*A*|*B*) = *P*(*B*|*A*) *P*(*A*)/*P*(*B*) (the probability of *A* given *B* is equal to the probability of *B* given *A* multiplied by the probability of *A* (independent of *B*) and divided by the probability of *B* (independent of *A*)).  In this context, *A* is the hypothesis (or the true value of the parameter) and *B* is the data (measured value).  *P*(*B*|*A*), the probability of the data, given the hypothesis, you should be able to calculate; *P*(*A*|*B*) is what you want to know.  *P*(*B*), the probability of obtaining this dataset summed over all possible hypotheses, is just a normalisation constant and can be dispensed with, but in order to get meaningful results you need to put something in for *P*(*A*), the **prior probability** of the hypothesis *A*, which represents your degree of belief in *A* before you did the experiment.  It is this need to specify *P*(*A*) that makes an analysis Bayesian &ndash; Bayes' theorem itself is an uncontroversial consequence of basic probability theory.

Bayesian statistics is becoming increasingly popular in many fields, partly because of the increasing availability of appropriate computational tools, such as **Markov Chain Monte Carlos**.  An advantage of Bayesian analysis is that it has a clean way of dealing with nuisance parameters by integrating the probability over them: *P*(**a**|**x**), where **a** is the set of meaningful parameters and **x** is the data, is given by &int;*P*(**a,v**|**x**)d**v**, where **v** is the set of nuisance parameters.  A disadvantage is that the choice of prior probability, *P*(*A*), is not clearly prescribed, and different experiments may get different results as a consequence of different choice of prior rather than real disagreement.  For this reason, the choice of priors needs to be carefully specified in reports of Bayesian analysis, and ideally the study of systematic errors should include an investigation of how much the final result depends on this.

##Summary

1. If you fit a function to a dataset, you should always assess the goodness of fit, by &chi;<sup>2</sup>, likelihood, KS test or [any appropriate test of the many that exist](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing).  If the function is *not* a good fit to the data, think very carefully before using the results of the fit.

2. In many cases, the nature of the problem compares a *null hypothesis* ("these data show nothing new") with an *alternative hypothesis* (e.g. "these data show evidence for a new particle").  The decision in this case is whether or not the data justify rejecting the null hypothesis, using an appropriate statistical test (see above).  It is essential to specify the problem carefully *before* looking at the data &ndash; choosing tests, cuts or rejection criteria *after* you know what the data look like invalidates your probability calculations and will probably bias your results.

3. Defining confidence intervals (error bars) is straightforward in the case of Gaussian distributions and data that are not close to unphysical regions.  Otherwise, it requires care, and there may be no simple answer ([this CDF note](http://www-cdf.fnal.gov/physics/statistics/notes/cdf6438_coverage.pdf) considers no less than 8 different schemes for the simple case of Poisson-distributed data without background, all of which give different results, none ideal).  The [Feldman-Cousins prescription](http://arxiv.org/abs/physics/9711021) may not be perfect, but it is something of an industry standard, at least in particle physics.

4. Bayesian techniques have some advantages, and are gaining in popularity, but introduce a dependence on the assumed prior probability which may cause equivalent experiments to report different results.  Unless there is a very clearly motivated best choice, the dependence on the prior is a systematic error and should be treated as such.